# Guia de Instala√ß√£o: Ollama + IA M√©dica

## üìã O que √© Ollama?

Ollama √© uma plataforma que permite rodar modelos de IA (LLMs) localmente no seu computador, **sem enviar dados para nuvem**. Ideal para sistemas de sa√∫de que precisam cumprir LGPD e manter privacidade total dos pacientes.

## üîí Seguran√ßa e Privacidade

- ‚úÖ **100% Local**: Dados nunca saem do servidor
- ‚úÖ **LGPD Compliant**: Sem transfer√™ncia internacional
- ‚úÖ **HL7 FHIR**: Compat√≠vel com padr√µes de sa√∫de
- ‚úÖ **Open Source**: C√≥digo audit√°vel

## üöÄ Instala√ß√£o no Windows

### Passo 1: Baixar Ollama

1. Acesse: https://ollama.ai/download
2. Clique em "Download for Windows"
3. Execute o instalador `OllamaSetup.exe`
4. Siga os passos da instala√ß√£o (Next ‚Üí Install ‚Üí Finish)

### Passo 2: Verificar Instala√ß√£o

Abra **PowerShell** ou **Terminal** e execute:

```powershell
ollama --version
```

Deve retornar algo como: `ollama version 0.1.x`

### Passo 3: Instalar Modelo de IA

Escolha **UM** dos modelos abaixo:

#### Op√ß√£o 1: Mistral (Recomendado para uso geral)

```powershell
ollama pull mistral
```

- **Tamanho**: ~4GB
- **Velocidade**: R√°pido
- **Qualidade**: Muito boa para resumos cl√≠nicos

#### Op√ß√£o 2: Medllama2 (Especializado em medicina)

```powershell
ollama pull medllama2
```

- **Tamanho**: ~3.8GB
- **Velocidade**: R√°pido
- **Qualidade**: Otimizado para textos m√©dicos

#### Op√ß√£o 3: Llama 3.2 (Mais avan√ßado)

```powershell
ollama pull llama3.2
```

- **Tamanho**: ~2GB
- **Velocidade**: Muito r√°pido
- **Qualidade**: Excelente

**Aguarde o download** (pode levar 5-15 minutos dependendo da internet).

### Passo 4: Verificar Modelos Instalados

```powershell
ollama list
```

Deve mostrar o modelo que voc√™ baixou:

```
NAME              ID              SIZE      MODIFIED
mistral:latest    abc123...       4.1 GB    2 minutes ago
```

### Passo 5: Iniciar Ollama (Autom√°tico no Windows)

Ollama inicia automaticamente ap√≥s a instala√ß√£o. Para verificar:

1. Abra **Gerenciador de Tarefas** (Ctrl+Shift+Esc)
2. Procure por "ollama" nos processos em segundo plano
3. Ou verifique a bandeja do sistema (√≠cone de lhama ü¶ô)

**Caso n√£o esteja rodando**, execute:

```powershell
ollama serve
```

### Passo 6: Testar Conex√£o

No PowerShell:

```powershell
curl http://localhost:11434/api/tags
```

Deve retornar JSON com a lista de modelos:

```json
{"models":[{"name":"mistral:latest",...}]}
```

## ‚öôÔ∏è Configura√ß√£o no OpenEHRCore

### Configurar Modelo no Backend

Edite `backend-django/openehrcore/settings.py` e adicione:

```python
# AI Configuration (Ollama)
OLLAMA_BASE_URL = 'http://localhost:11434'
OLLAMA_MODEL = 'mistral'  # ou 'medllama2' ou 'llama3.2'
```

### Reiniciar Django

```powershell
cd backend-django
python manage.py runserver
```

### Testar Resumo de IA

1. Acesse o sistema: http://localhost:5173
2. Abra um paciente
3. O resumo de IA aparecer√° no topo com indicador "Ollama Ativo" üü¢

## üß™ Teste Manual via API

```powershell
$headers = @{ 'Authorization' = 'Bearer dev-token-bypass' }
Invoke-RestMethod http://localhost:8000/api/v1/ai/summary/860/ -Headers $headers | Select-Object -ExpandProperty summary
```

Deve retornar um resumo cl√≠nico gerado por IA.

## ‚ùó Solu√ß√£o de Problemas

### "Ollama n√£o conectou"

**Causa**: Ollama n√£o est√° rodando.

**Solu√ß√£o**:

1. Abra o aplicativo Ollama (√≠cone de lhama na bandeja)
2. Ou execute: `ollama serve`

### "Modelo 'mistral' n√£o encontrado"

**Causa**: Modelo n√£o foi baixado.

**Solu√ß√£o**:

```powershell
ollama pull mistral
ollama list  # Verificar se aparece
```

### "Timeout ao gerar resumo"

**Causa**: Modelo muito grande para CPU.

**Solu√ß√£o**: Use um modelo menor:

```powershell
ollama pull llama3.2  # Apenas 2GB, mais r√°pido
```

E atualize `settings.py`:

```python
OLLAMA_MODEL = 'llama3.2'
```

### "Resumo com erros de portugu√™s"

**Causa**: Modelo em ingl√™s.

**Solu√ß√£o**: Adicione instru√ß√£o mais clara no prompt (j√° configurado no c√≥digo).

## üìä Recursos de Hardware

| Modelo    | RAM M√≠nima | RAM Recomendada | CPU      |
| --------- | ---------- | --------------- | -------- |
| llama3.2  | 4GB        | 8GB             | Qualquer |
| mistral   | 8GB        | 16GB            | 4+ cores |
| medllama2 | 8GB        | 16GB            | 4+ cores |

**Dica**: Para produ√ß√£o com m√∫ltiplos usu√°rios, considere:

- 16GB+ RAM
- SSD (melhora tempo de carregamento)
- CPU moderna (Intel i5/Ryzen 5+)

## üîß Comandos √öteis

```powershell
# Listar modelos instalados
ollama list

# Remover modelo (liberar espa√ßo)
ollama rm mistral

# Ver logs do Ollama
ollama logs

# Parar Ollama
Stop-Process -Name ollama

# Iniciar Ollama
ollama serve

# Testar modelo diretamente
ollama run mistral "Resuma: Paciente diab√©tico tipo 2, hipertenso"
```

## üåê Instala√ß√£o em Linux/Mac

### Ubuntu/Debian

```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull mistral
```

### macOS

```bash
brew install ollama
ollama pull mistral
```

## üìö Pr√≥ximos Passos

Ap√≥s instalar Ollama:

1. ‚úÖ Reinicie Django
2. ‚úÖ Acesse um paciente no sistema
3. ‚úÖ Verifique o indicador "Ollama Ativo" no resumo
4. ‚úÖ O resumo ser√° gerado por IA em vez do fallback estruturado

**Pronto!** Agora o sistema usa IA local com total privacidade e seguran√ßa.

---

## üÜò Suporte

- Documenta√ß√£o Ollama: https://github.com/ollama/ollama
- Issues: Abra ticket no GitHub do projeto
- Discord Ollama: https://discord.gg/ollama
